{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe6281acf78>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                         train=True,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)\n",
    "\n",
    "# Data loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                          batch_size=100, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_var(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n",
    "\n",
    "# VAE model\n",
    "class LinearVAE(nn.Module):\n",
    "    def __init__(self, image_size=784, h_dim=400, z_dim=20):\n",
    "        super(LinearVAE, self).__init__()\n",
    "        self.linear1 = nn.Linear(image_size, h_dim)\n",
    "        self.linear2 = nn.Linear(h_dim, z_dim)\n",
    "        self.linear3 = nn.Linear(z_dim, h_dim)\n",
    "        self.linear4 = nn.Linear(h_dim, 784)\n",
    "    \n",
    "    def encoder(self, x):\n",
    "        x = F.relu(self.linear1(x.view(-1, 28 * 28)))\n",
    "        z_mean = self.linear2(x)\n",
    "        z_log_var = self.linear2(x)\n",
    "        return z_mean, z_log_var\n",
    "    \n",
    "    def decoder(self, z):\n",
    "        z = F.relu(self.linear3(z))\n",
    "        recon_x = F.sigmoid(self.linear4(z))\n",
    "        return recon_x\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\"z = mean + eps * sigma where eps is sampled from N(0, 1).\"\"\"\n",
    "        eps = to_var(torch.randn(mu.size(0), mu.size(1)))\n",
    "        z = mu + eps * torch.exp(log_var/2)    # 2 for convert var to std\n",
    "        return z\n",
    "                     \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x) # torch.chunk(h, 2, dim=1)  # mean and log variance.\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        out = self.decoder(z)\n",
    "        return out, mu, log_var\n",
    "    \n",
    "    def sample(self, z):\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/50], Step [1/600], Total Loss: 55014.4961, Reconst Loss: 55004.5234, KL Div: 9.9731340\n",
      "Epoch[1/50], Step [101/600], Total Loss: 19750.7656, Reconst Loss: 18899.0703, KL Div: 851.6956177\n",
      "Epoch[1/50], Step [201/600], Total Loss: 16937.0508, Reconst Loss: 15538.3174, KL Div: 1398.7341309\n",
      "Epoch[1/50], Step [301/600], Total Loss: 16740.2148, Reconst Loss: 14925.6260, KL Div: 1814.5885010\n",
      "Epoch[1/50], Step [401/600], Total Loss: 15652.4707, Reconst Loss: 13794.6904, KL Div: 1857.7805176\n",
      "Epoch[1/50], Step [501/600], Total Loss: 15243.4199, Reconst Loss: 13151.3213, KL Div: 2092.0991211\n",
      "Epoch[2/50], Step [1/600], Total Loss: 15273.3564, Reconst Loss: 13122.3945, KL Div: 2150.9619141\n",
      "Epoch[2/50], Step [101/600], Total Loss: 14789.0986, Reconst Loss: 12573.1396, KL Div: 2215.9587402\n",
      "Epoch[2/50], Step [201/600], Total Loss: 15081.9258, Reconst Loss: 12814.1484, KL Div: 2267.7775879\n",
      "Epoch[2/50], Step [301/600], Total Loss: 13760.2021, Reconst Loss: 11583.2031, KL Div: 2176.9992676\n",
      "Epoch[2/50], Step [401/600], Total Loss: 14441.3701, Reconst Loss: 11970.1963, KL Div: 2471.1738281\n",
      "Epoch[2/50], Step [501/600], Total Loss: 14475.6650, Reconst Loss: 12087.5000, KL Div: 2388.1652832\n",
      "Epoch[3/50], Step [1/600], Total Loss: 13983.8906, Reconst Loss: 11586.6426, KL Div: 2397.2480469\n",
      "Epoch[3/50], Step [101/600], Total Loss: 13770.9180, Reconst Loss: 11161.2461, KL Div: 2609.6716309\n",
      "Epoch[3/50], Step [201/600], Total Loss: 14712.2051, Reconst Loss: 12240.6787, KL Div: 2471.5261230\n",
      "Epoch[3/50], Step [301/600], Total Loss: 14222.4902, Reconst Loss: 11720.4160, KL Div: 2502.0739746\n",
      "Epoch[3/50], Step [401/600], Total Loss: 13775.4014, Reconst Loss: 11046.7363, KL Div: 2728.6647949\n",
      "Epoch[3/50], Step [501/600], Total Loss: 13684.5859, Reconst Loss: 11255.5371, KL Div: 2429.0483398\n",
      "Epoch[4/50], Step [1/600], Total Loss: 14158.2646, Reconst Loss: 11653.3223, KL Div: 2504.9423828\n",
      "Epoch[4/50], Step [101/600], Total Loss: 13734.5586, Reconst Loss: 11084.8477, KL Div: 2649.7104492\n",
      "Epoch[4/50], Step [201/600], Total Loss: 13573.0283, Reconst Loss: 11102.0947, KL Div: 2470.9335938\n",
      "Epoch[4/50], Step [301/600], Total Loss: 13369.9053, Reconst Loss: 10867.1934, KL Div: 2502.7121582\n",
      "Epoch[4/50], Step [401/600], Total Loss: 13382.3359, Reconst Loss: 10875.8076, KL Div: 2506.5288086\n",
      "Epoch[4/50], Step [501/600], Total Loss: 13824.1201, Reconst Loss: 11108.4238, KL Div: 2715.6962891\n",
      "Epoch[5/50], Step [1/600], Total Loss: 13220.1895, Reconst Loss: 10593.6719, KL Div: 2626.5180664\n",
      "Epoch[5/50], Step [101/600], Total Loss: 13169.4912, Reconst Loss: 10682.8398, KL Div: 2486.6516113\n",
      "Epoch[5/50], Step [201/600], Total Loss: 13095.1396, Reconst Loss: 10475.1689, KL Div: 2619.9707031\n",
      "Epoch[5/50], Step [301/600], Total Loss: 12951.7715, Reconst Loss: 10432.3359, KL Div: 2519.4355469\n",
      "Epoch[5/50], Step [401/600], Total Loss: 13940.2207, Reconst Loss: 11423.5312, KL Div: 2516.6894531\n",
      "Epoch[5/50], Step [501/600], Total Loss: 13389.1074, Reconst Loss: 10827.4990, KL Div: 2561.6081543\n",
      "Epoch[6/50], Step [1/600], Total Loss: 13307.4922, Reconst Loss: 10704.3418, KL Div: 2603.1503906\n",
      "Epoch[6/50], Step [101/600], Total Loss: 13526.5029, Reconst Loss: 10947.5947, KL Div: 2578.9079590\n",
      "Epoch[6/50], Step [201/600], Total Loss: 12842.3594, Reconst Loss: 10346.3604, KL Div: 2495.9990234\n",
      "Epoch[6/50], Step [301/600], Total Loss: 13566.7227, Reconst Loss: 10759.3682, KL Div: 2807.3547363\n",
      "Epoch[6/50], Step [401/600], Total Loss: 12869.4453, Reconst Loss: 10296.1240, KL Div: 2573.3212891\n",
      "Epoch[6/50], Step [501/600], Total Loss: 14045.9893, Reconst Loss: 11232.5400, KL Div: 2813.4489746\n",
      "Epoch[7/50], Step [1/600], Total Loss: 13468.2646, Reconst Loss: 10728.9414, KL Div: 2739.3232422\n",
      "Epoch[7/50], Step [101/600], Total Loss: 13305.9795, Reconst Loss: 10581.3398, KL Div: 2724.6396484\n",
      "Epoch[7/50], Step [201/600], Total Loss: 13037.5928, Reconst Loss: 10284.4102, KL Div: 2753.1826172\n",
      "Epoch[7/50], Step [301/600], Total Loss: 13534.2266, Reconst Loss: 10923.3994, KL Div: 2610.8276367\n",
      "Epoch[7/50], Step [401/600], Total Loss: 13971.3086, Reconst Loss: 11174.5703, KL Div: 2796.7385254\n",
      "Epoch[7/50], Step [501/600], Total Loss: 13541.6484, Reconst Loss: 10846.2871, KL Div: 2695.3613281\n",
      "Epoch[8/50], Step [1/600], Total Loss: 13485.4805, Reconst Loss: 10857.6211, KL Div: 2627.8588867\n",
      "Epoch[8/50], Step [101/600], Total Loss: 13400.8428, Reconst Loss: 10788.5967, KL Div: 2612.2460938\n",
      "Epoch[8/50], Step [201/600], Total Loss: 13782.8350, Reconst Loss: 10933.6934, KL Div: 2849.1413574\n",
      "Epoch[8/50], Step [301/600], Total Loss: 13460.0898, Reconst Loss: 10741.8936, KL Div: 2718.1962891\n",
      "Epoch[8/50], Step [401/600], Total Loss: 13722.3848, Reconst Loss: 10988.3682, KL Div: 2734.0166016\n",
      "Epoch[8/50], Step [501/600], Total Loss: 13547.3398, Reconst Loss: 10922.5762, KL Div: 2624.7639160\n",
      "Epoch[9/50], Step [1/600], Total Loss: 13080.1191, Reconst Loss: 10574.0254, KL Div: 2506.0942383\n",
      "Epoch[9/50], Step [101/600], Total Loss: 13605.4062, Reconst Loss: 11142.8604, KL Div: 2462.5456543\n",
      "Epoch[9/50], Step [201/600], Total Loss: 12650.8545, Reconst Loss: 10194.2900, KL Div: 2456.5642090\n",
      "Epoch[9/50], Step [301/600], Total Loss: 12877.1855, Reconst Loss: 10215.2617, KL Div: 2661.9238281\n",
      "Epoch[9/50], Step [401/600], Total Loss: 13601.4385, Reconst Loss: 10942.0586, KL Div: 2659.3801270\n",
      "Epoch[9/50], Step [501/600], Total Loss: 12986.0732, Reconst Loss: 10500.4053, KL Div: 2485.6682129\n",
      "Epoch[10/50], Step [1/600], Total Loss: 13254.8115, Reconst Loss: 10543.3838, KL Div: 2711.4277344\n",
      "Epoch[10/50], Step [101/600], Total Loss: 12819.6270, Reconst Loss: 10202.8125, KL Div: 2616.8146973\n",
      "Epoch[10/50], Step [201/600], Total Loss: 13863.5254, Reconst Loss: 11047.6162, KL Div: 2815.9094238\n",
      "Epoch[10/50], Step [301/600], Total Loss: 14065.2100, Reconst Loss: 11232.2949, KL Div: 2832.9152832\n",
      "Epoch[10/50], Step [401/600], Total Loss: 13384.6934, Reconst Loss: 10692.7734, KL Div: 2691.9201660\n",
      "Epoch[10/50], Step [501/600], Total Loss: 13259.7939, Reconst Loss: 10505.6191, KL Div: 2754.1745605\n",
      "Epoch[11/50], Step [1/600], Total Loss: 13016.3701, Reconst Loss: 10222.3701, KL Div: 2794.0002441\n",
      "Epoch[11/50], Step [101/600], Total Loss: 13166.0430, Reconst Loss: 10607.0107, KL Div: 2559.0327148\n",
      "Epoch[11/50], Step [201/600], Total Loss: 13267.7158, Reconst Loss: 10499.6924, KL Div: 2768.0231934\n",
      "Epoch[11/50], Step [301/600], Total Loss: 12719.4561, Reconst Loss: 10202.1963, KL Div: 2517.2597656\n",
      "Epoch[11/50], Step [401/600], Total Loss: 12593.3789, Reconst Loss: 10001.2910, KL Div: 2592.0881348\n",
      "Epoch[11/50], Step [501/600], Total Loss: 13483.2754, Reconst Loss: 10851.1934, KL Div: 2632.0820312\n",
      "Epoch[12/50], Step [1/600], Total Loss: 13304.0566, Reconst Loss: 10637.1309, KL Div: 2666.9260254\n",
      "Epoch[12/50], Step [101/600], Total Loss: 13013.4766, Reconst Loss: 10290.1650, KL Div: 2723.3120117\n",
      "Epoch[12/50], Step [201/600], Total Loss: 13371.6221, Reconst Loss: 10524.9834, KL Div: 2846.6389160\n",
      "Epoch[12/50], Step [301/600], Total Loss: 13469.5234, Reconst Loss: 10638.8916, KL Div: 2830.6320801\n",
      "Epoch[12/50], Step [401/600], Total Loss: 12713.1211, Reconst Loss: 10100.6377, KL Div: 2612.4836426\n",
      "Epoch[12/50], Step [501/600], Total Loss: 12451.2383, Reconst Loss: 9815.9727, KL Div: 2635.2656250\n",
      "Epoch[13/50], Step [1/600], Total Loss: 13138.0107, Reconst Loss: 10410.9023, KL Div: 2727.1083984\n",
      "Epoch[13/50], Step [101/600], Total Loss: 13384.4756, Reconst Loss: 10694.8076, KL Div: 2689.6679688\n",
      "Epoch[13/50], Step [201/600], Total Loss: 13177.0371, Reconst Loss: 10338.6250, KL Div: 2838.4118652\n",
      "Epoch[13/50], Step [301/600], Total Loss: 12575.2441, Reconst Loss: 9826.8223, KL Div: 2748.4216309\n",
      "Epoch[13/50], Step [401/600], Total Loss: 13012.1631, Reconst Loss: 10490.9688, KL Div: 2521.1943359\n",
      "Epoch[13/50], Step [501/600], Total Loss: 13454.6592, Reconst Loss: 10770.8760, KL Div: 2683.7834473\n",
      "Epoch[14/50], Step [1/600], Total Loss: 13608.9395, Reconst Loss: 10720.4697, KL Div: 2888.4694824\n",
      "Epoch[14/50], Step [101/600], Total Loss: 13742.5293, Reconst Loss: 11122.5693, KL Div: 2619.9594727\n",
      "Epoch[14/50], Step [201/600], Total Loss: 13313.1768, Reconst Loss: 10718.1621, KL Div: 2595.0148926\n",
      "Epoch[14/50], Step [301/600], Total Loss: 13727.4600, Reconst Loss: 10920.7012, KL Div: 2806.7585449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[14/50], Step [401/600], Total Loss: 12035.3916, Reconst Loss: 9454.2236, KL Div: 2581.1677246\n",
      "Epoch[14/50], Step [501/600], Total Loss: 12843.8887, Reconst Loss: 10243.4277, KL Div: 2600.4609375\n",
      "Epoch[15/50], Step [1/600], Total Loss: 12785.8125, Reconst Loss: 10126.5410, KL Div: 2659.2714844\n",
      "Epoch[15/50], Step [101/600], Total Loss: 12760.8887, Reconst Loss: 10143.3965, KL Div: 2617.4921875\n",
      "Epoch[15/50], Step [201/600], Total Loss: 13222.0977, Reconst Loss: 10345.2109, KL Div: 2876.8864746\n",
      "Epoch[15/50], Step [301/600], Total Loss: 13480.0098, Reconst Loss: 10668.7920, KL Div: 2811.2172852\n",
      "Epoch[15/50], Step [401/600], Total Loss: 13197.3887, Reconst Loss: 10623.4902, KL Div: 2573.8986816\n",
      "Epoch[15/50], Step [501/600], Total Loss: 12778.7998, Reconst Loss: 10051.1367, KL Div: 2727.6633301\n",
      "Epoch[16/50], Step [1/600], Total Loss: 12893.8945, Reconst Loss: 10210.1084, KL Div: 2683.7856445\n",
      "Epoch[16/50], Step [101/600], Total Loss: 12732.3730, Reconst Loss: 10038.7021, KL Div: 2693.6711426\n",
      "Epoch[16/50], Step [201/600], Total Loss: 12522.3936, Reconst Loss: 9887.6191, KL Div: 2634.7744141\n",
      "Epoch[16/50], Step [301/600], Total Loss: 12848.4297, Reconst Loss: 10144.2676, KL Div: 2704.1618652\n",
      "Epoch[16/50], Step [401/600], Total Loss: 12897.8428, Reconst Loss: 10368.0811, KL Div: 2529.7619629\n",
      "Epoch[16/50], Step [501/600], Total Loss: 12954.8652, Reconst Loss: 10115.5605, KL Div: 2839.3051758\n",
      "Epoch[17/50], Step [1/600], Total Loss: 12956.3379, Reconst Loss: 10288.2539, KL Div: 2668.0844727\n",
      "Epoch[17/50], Step [101/600], Total Loss: 13041.1680, Reconst Loss: 10255.7461, KL Div: 2785.4218750\n",
      "Epoch[17/50], Step [201/600], Total Loss: 13501.6465, Reconst Loss: 10728.1084, KL Div: 2773.5385742\n",
      "Epoch[17/50], Step [301/600], Total Loss: 13504.0371, Reconst Loss: 10733.5850, KL Div: 2770.4516602\n",
      "Epoch[17/50], Step [401/600], Total Loss: 13290.3984, Reconst Loss: 10596.0332, KL Div: 2694.3647461\n",
      "Epoch[17/50], Step [501/600], Total Loss: 12860.7383, Reconst Loss: 10155.0947, KL Div: 2705.6440430\n",
      "Epoch[18/50], Step [1/600], Total Loss: 12827.0576, Reconst Loss: 10128.9404, KL Div: 2698.1171875\n",
      "Epoch[18/50], Step [101/600], Total Loss: 12598.2842, Reconst Loss: 9915.4102, KL Div: 2682.8737793\n",
      "Epoch[18/50], Step [201/600], Total Loss: 12624.6709, Reconst Loss: 9930.9883, KL Div: 2693.6826172\n",
      "Epoch[18/50], Step [301/600], Total Loss: 13113.8076, Reconst Loss: 10356.9824, KL Div: 2756.8249512\n",
      "Epoch[18/50], Step [401/600], Total Loss: 12716.7852, Reconst Loss: 10053.7559, KL Div: 2663.0295410\n",
      "Epoch[18/50], Step [501/600], Total Loss: 13587.3906, Reconst Loss: 10797.1777, KL Div: 2790.2124023\n",
      "Epoch[19/50], Step [1/600], Total Loss: 13125.2754, Reconst Loss: 10493.4512, KL Div: 2631.8237305\n",
      "Epoch[19/50], Step [101/600], Total Loss: 13065.0303, Reconst Loss: 10261.0312, KL Div: 2803.9987793\n",
      "Epoch[19/50], Step [201/600], Total Loss: 13158.5742, Reconst Loss: 10229.3516, KL Div: 2929.2229004\n",
      "Epoch[19/50], Step [301/600], Total Loss: 12374.1416, Reconst Loss: 9812.3389, KL Div: 2561.8029785\n",
      "Epoch[19/50], Step [401/600], Total Loss: 12032.4189, Reconst Loss: 9477.6924, KL Div: 2554.7265625\n",
      "Epoch[19/50], Step [501/600], Total Loss: 13321.7529, Reconst Loss: 10490.6387, KL Div: 2831.1142578\n",
      "Epoch[20/50], Step [1/600], Total Loss: 12953.4277, Reconst Loss: 10207.5908, KL Div: 2745.8366699\n",
      "Epoch[20/50], Step [101/600], Total Loss: 12957.6426, Reconst Loss: 10296.6846, KL Div: 2660.9577637\n",
      "Epoch[20/50], Step [201/600], Total Loss: 12938.1758, Reconst Loss: 10160.6523, KL Div: 2777.5234375\n",
      "Epoch[20/50], Step [301/600], Total Loss: 12755.0547, Reconst Loss: 10094.1914, KL Div: 2660.8627930\n",
      "Epoch[20/50], Step [401/600], Total Loss: 12805.5508, Reconst Loss: 10104.0957, KL Div: 2701.4545898\n",
      "Epoch[20/50], Step [501/600], Total Loss: 13055.2168, Reconst Loss: 10338.1777, KL Div: 2717.0390625\n",
      "Epoch[21/50], Step [1/600], Total Loss: 13696.4863, Reconst Loss: 11022.3477, KL Div: 2674.1391602\n",
      "Epoch[21/50], Step [101/600], Total Loss: 12202.4395, Reconst Loss: 9616.2061, KL Div: 2586.2336426\n",
      "Epoch[21/50], Step [201/600], Total Loss: 13194.2344, Reconst Loss: 10474.6895, KL Div: 2719.5451660\n",
      "Epoch[21/50], Step [301/600], Total Loss: 13358.4922, Reconst Loss: 10656.6133, KL Div: 2701.8789062\n",
      "Epoch[21/50], Step [401/600], Total Loss: 12955.8564, Reconst Loss: 10343.4023, KL Div: 2612.4541016\n",
      "Epoch[21/50], Step [501/600], Total Loss: 13672.9893, Reconst Loss: 10737.2021, KL Div: 2935.7871094\n",
      "Epoch[22/50], Step [1/600], Total Loss: 12847.7793, Reconst Loss: 10223.6592, KL Div: 2624.1206055\n",
      "Epoch[22/50], Step [101/600], Total Loss: 12946.9180, Reconst Loss: 10286.8779, KL Div: 2660.0397949\n",
      "Epoch[22/50], Step [201/600], Total Loss: 13255.2861, Reconst Loss: 10357.3398, KL Div: 2897.9465332\n",
      "Epoch[22/50], Step [301/600], Total Loss: 12539.7490, Reconst Loss: 9882.2168, KL Div: 2657.5319824\n",
      "Epoch[22/50], Step [401/600], Total Loss: 12815.1152, Reconst Loss: 10082.4727, KL Div: 2732.6425781\n",
      "Epoch[22/50], Step [501/600], Total Loss: 12639.3691, Reconst Loss: 10126.2344, KL Div: 2513.1350098\n",
      "Epoch[23/50], Step [1/600], Total Loss: 12708.4873, Reconst Loss: 10111.3418, KL Div: 2597.1455078\n",
      "Epoch[23/50], Step [101/600], Total Loss: 12504.3779, Reconst Loss: 9869.4023, KL Div: 2634.9753418\n",
      "Epoch[23/50], Step [201/600], Total Loss: 13229.6211, Reconst Loss: 10375.7529, KL Div: 2853.8679199\n",
      "Epoch[23/50], Step [301/600], Total Loss: 12745.1270, Reconst Loss: 10248.4990, KL Div: 2496.6276855\n",
      "Epoch[23/50], Step [401/600], Total Loss: 13156.0918, Reconst Loss: 10353.6621, KL Div: 2802.4301758\n",
      "Epoch[23/50], Step [501/600], Total Loss: 13054.2432, Reconst Loss: 10243.5391, KL Div: 2810.7041016\n",
      "Epoch[24/50], Step [1/600], Total Loss: 12782.7324, Reconst Loss: 10209.8184, KL Div: 2572.9135742\n",
      "Epoch[24/50], Step [101/600], Total Loss: 13171.5879, Reconst Loss: 10495.2842, KL Div: 2676.3032227\n",
      "Epoch[24/50], Step [201/600], Total Loss: 13168.5273, Reconst Loss: 10262.4717, KL Div: 2906.0551758\n",
      "Epoch[24/50], Step [301/600], Total Loss: 13354.5146, Reconst Loss: 10406.0605, KL Div: 2948.4543457\n",
      "Epoch[24/50], Step [401/600], Total Loss: 13015.9229, Reconst Loss: 10237.0703, KL Div: 2778.8522949\n",
      "Epoch[24/50], Step [501/600], Total Loss: 12680.0010, Reconst Loss: 9994.9805, KL Div: 2685.0202637\n",
      "Epoch[25/50], Step [1/600], Total Loss: 12819.6992, Reconst Loss: 10127.3818, KL Div: 2692.3173828\n",
      "Epoch[25/50], Step [101/600], Total Loss: 12858.7539, Reconst Loss: 10127.6445, KL Div: 2731.1098633\n",
      "Epoch[25/50], Step [201/600], Total Loss: 12623.5020, Reconst Loss: 10032.1201, KL Div: 2591.3813477\n",
      "Epoch[25/50], Step [301/600], Total Loss: 12817.1396, Reconst Loss: 10282.9180, KL Div: 2534.2216797\n",
      "Epoch[25/50], Step [401/600], Total Loss: 13255.7158, Reconst Loss: 10658.8027, KL Div: 2596.9133301\n",
      "Epoch[25/50], Step [501/600], Total Loss: 13311.1074, Reconst Loss: 10615.8447, KL Div: 2695.2626953\n",
      "Epoch[26/50], Step [1/600], Total Loss: 13134.7939, Reconst Loss: 10485.5811, KL Div: 2649.2126465\n",
      "Epoch[26/50], Step [101/600], Total Loss: 12541.6309, Reconst Loss: 9856.9805, KL Div: 2684.6499023\n",
      "Epoch[26/50], Step [201/600], Total Loss: 12899.9434, Reconst Loss: 10177.2139, KL Div: 2722.7299805\n",
      "Epoch[26/50], Step [301/600], Total Loss: 12736.5830, Reconst Loss: 9925.8594, KL Div: 2810.7238770\n",
      "Epoch[26/50], Step [401/600], Total Loss: 12746.9277, Reconst Loss: 10032.3447, KL Div: 2714.5832520\n",
      "Epoch[26/50], Step [501/600], Total Loss: 13371.2910, Reconst Loss: 10617.3105, KL Div: 2753.9809570\n",
      "Epoch[27/50], Step [1/600], Total Loss: 12987.1836, Reconst Loss: 10334.1738, KL Div: 2653.0095215\n",
      "Epoch[27/50], Step [101/600], Total Loss: 12921.9844, Reconst Loss: 10218.7764, KL Div: 2703.2082520\n",
      "Epoch[27/50], Step [201/600], Total Loss: 12543.8516, Reconst Loss: 9837.2666, KL Div: 2706.5844727\n",
      "Epoch[27/50], Step [301/600], Total Loss: 13076.6191, Reconst Loss: 10395.7461, KL Div: 2680.8725586\n",
      "Epoch[27/50], Step [401/600], Total Loss: 13216.5996, Reconst Loss: 10443.0840, KL Div: 2773.5161133\n",
      "Epoch[27/50], Step [501/600], Total Loss: 12589.2891, Reconst Loss: 9867.6338, KL Div: 2721.6550293\n",
      "Epoch[28/50], Step [1/600], Total Loss: 12616.6387, Reconst Loss: 10031.7510, KL Div: 2584.8874512\n",
      "Epoch[28/50], Step [101/600], Total Loss: 12735.7607, Reconst Loss: 10176.7402, KL Div: 2559.0205078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[28/50], Step [201/600], Total Loss: 12938.3232, Reconst Loss: 10142.3096, KL Div: 2796.0136719\n",
      "Epoch[28/50], Step [301/600], Total Loss: 12757.0449, Reconst Loss: 10146.5615, KL Div: 2610.4831543\n",
      "Epoch[28/50], Step [401/600], Total Loss: 12875.3535, Reconst Loss: 10225.3047, KL Div: 2650.0488281\n",
      "Epoch[28/50], Step [501/600], Total Loss: 13061.0293, Reconst Loss: 10451.2451, KL Div: 2609.7846680\n",
      "Epoch[29/50], Step [1/600], Total Loss: 12956.7695, Reconst Loss: 10242.5127, KL Div: 2714.2570801\n",
      "Epoch[29/50], Step [101/600], Total Loss: 12595.5918, Reconst Loss: 9897.7070, KL Div: 2697.8845215\n",
      "Epoch[29/50], Step [201/600], Total Loss: 12927.9512, Reconst Loss: 10122.5156, KL Div: 2805.4353027\n",
      "Epoch[29/50], Step [301/600], Total Loss: 13048.3809, Reconst Loss: 10135.8301, KL Div: 2912.5507812\n",
      "Epoch[29/50], Step [401/600], Total Loss: 12341.0234, Reconst Loss: 9700.5840, KL Div: 2640.4389648\n",
      "Epoch[29/50], Step [501/600], Total Loss: 13058.6035, Reconst Loss: 10467.6074, KL Div: 2590.9960938\n",
      "Epoch[30/50], Step [1/600], Total Loss: 12726.1152, Reconst Loss: 9878.0654, KL Div: 2848.0502930\n",
      "Epoch[30/50], Step [101/600], Total Loss: 13024.2949, Reconst Loss: 10228.8154, KL Div: 2795.4790039\n",
      "Epoch[30/50], Step [201/600], Total Loss: 12325.5869, Reconst Loss: 9600.1377, KL Div: 2725.4492188\n",
      "Epoch[30/50], Step [301/600], Total Loss: 13454.5293, Reconst Loss: 10517.7422, KL Div: 2936.7868652\n",
      "Epoch[30/50], Step [401/600], Total Loss: 12246.8359, Reconst Loss: 9803.1436, KL Div: 2443.6926270\n",
      "Epoch[30/50], Step [501/600], Total Loss: 13309.9062, Reconst Loss: 10447.1416, KL Div: 2862.7641602\n",
      "Epoch[31/50], Step [1/600], Total Loss: 12317.9072, Reconst Loss: 9777.5791, KL Div: 2540.3281250\n",
      "Epoch[31/50], Step [101/600], Total Loss: 12620.1533, Reconst Loss: 9873.2920, KL Div: 2746.8615723\n",
      "Epoch[31/50], Step [201/600], Total Loss: 12926.8623, Reconst Loss: 10200.9873, KL Div: 2725.8747559\n",
      "Epoch[31/50], Step [301/600], Total Loss: 12765.6396, Reconst Loss: 10217.7920, KL Div: 2547.8474121\n",
      "Epoch[31/50], Step [401/600], Total Loss: 12604.8447, Reconst Loss: 9937.1084, KL Div: 2667.7360840\n",
      "Epoch[31/50], Step [501/600], Total Loss: 12649.8633, Reconst Loss: 10086.8252, KL Div: 2563.0383301\n",
      "Epoch[32/50], Step [1/600], Total Loss: 13135.8613, Reconst Loss: 10377.7197, KL Div: 2758.1416016\n",
      "Epoch[32/50], Step [101/600], Total Loss: 12534.8750, Reconst Loss: 9956.2021, KL Div: 2578.6730957\n",
      "Epoch[32/50], Step [201/600], Total Loss: 12573.9756, Reconst Loss: 9865.5049, KL Div: 2708.4704590\n",
      "Epoch[32/50], Step [301/600], Total Loss: 13209.6426, Reconst Loss: 10370.5791, KL Div: 2839.0632324\n",
      "Epoch[32/50], Step [401/600], Total Loss: 12683.6836, Reconst Loss: 10079.5859, KL Div: 2604.0981445\n",
      "Epoch[32/50], Step [501/600], Total Loss: 12913.5176, Reconst Loss: 10221.2080, KL Div: 2692.3100586\n",
      "Epoch[33/50], Step [1/600], Total Loss: 12767.1660, Reconst Loss: 10030.4180, KL Div: 2736.7478027\n",
      "Epoch[33/50], Step [101/600], Total Loss: 12778.4834, Reconst Loss: 10039.9219, KL Div: 2738.5615234\n",
      "Epoch[33/50], Step [201/600], Total Loss: 12835.3418, Reconst Loss: 10155.8193, KL Div: 2679.5224609\n",
      "Epoch[33/50], Step [301/600], Total Loss: 12418.8291, Reconst Loss: 9710.2480, KL Div: 2708.5812988\n",
      "Epoch[33/50], Step [401/600], Total Loss: 12751.1035, Reconst Loss: 10011.7109, KL Div: 2739.3930664\n",
      "Epoch[33/50], Step [501/600], Total Loss: 12639.9463, Reconst Loss: 10003.7031, KL Div: 2636.2434082\n",
      "Epoch[34/50], Step [1/600], Total Loss: 12771.9980, Reconst Loss: 10074.4971, KL Div: 2697.5014648\n",
      "Epoch[34/50], Step [101/600], Total Loss: 13423.2920, Reconst Loss: 10529.1357, KL Div: 2894.1560059\n",
      "Epoch[34/50], Step [201/600], Total Loss: 12761.0996, Reconst Loss: 9993.2930, KL Div: 2767.8071289\n",
      "Epoch[34/50], Step [301/600], Total Loss: 12916.5039, Reconst Loss: 10312.6758, KL Div: 2603.8281250\n",
      "Epoch[34/50], Step [401/600], Total Loss: 12967.3906, Reconst Loss: 10330.5977, KL Div: 2636.7929688\n",
      "Epoch[34/50], Step [501/600], Total Loss: 12847.1035, Reconst Loss: 10083.2520, KL Div: 2763.8510742\n",
      "Epoch[35/50], Step [1/600], Total Loss: 12860.6162, Reconst Loss: 10093.1816, KL Div: 2767.4348145\n",
      "Epoch[35/50], Step [101/600], Total Loss: 13098.8984, Reconst Loss: 10370.9121, KL Div: 2727.9863281\n",
      "Epoch[35/50], Step [201/600], Total Loss: 12822.3926, Reconst Loss: 10145.6240, KL Div: 2676.7685547\n",
      "Epoch[35/50], Step [301/600], Total Loss: 12938.9854, Reconst Loss: 10166.6504, KL Div: 2772.3349609\n",
      "Epoch[35/50], Step [401/600], Total Loss: 12450.5840, Reconst Loss: 9850.4268, KL Div: 2600.1572266\n",
      "Epoch[35/50], Step [501/600], Total Loss: 12709.3750, Reconst Loss: 10086.0508, KL Div: 2623.3237305\n",
      "Epoch[36/50], Step [1/600], Total Loss: 13030.8252, Reconst Loss: 10269.9141, KL Div: 2760.9108887\n",
      "Epoch[36/50], Step [101/600], Total Loss: 13086.4629, Reconst Loss: 10283.3379, KL Div: 2803.1245117\n",
      "Epoch[36/50], Step [201/600], Total Loss: 12052.5430, Reconst Loss: 9546.7480, KL Div: 2505.7949219\n",
      "Epoch[36/50], Step [301/600], Total Loss: 11685.9355, Reconst Loss: 9024.2549, KL Div: 2661.6806641\n",
      "Epoch[36/50], Step [401/600], Total Loss: 13106.0371, Reconst Loss: 10327.5586, KL Div: 2778.4790039\n",
      "Epoch[36/50], Step [501/600], Total Loss: 12717.0117, Reconst Loss: 9997.6816, KL Div: 2719.3295898\n",
      "Epoch[37/50], Step [1/600], Total Loss: 13077.7236, Reconst Loss: 10243.2979, KL Div: 2834.4255371\n",
      "Epoch[37/50], Step [101/600], Total Loss: 13226.6553, Reconst Loss: 10459.0078, KL Div: 2767.6472168\n",
      "Epoch[37/50], Step [201/600], Total Loss: 12584.8799, Reconst Loss: 9970.9443, KL Div: 2613.9357910\n",
      "Epoch[37/50], Step [301/600], Total Loss: 12727.2480, Reconst Loss: 10089.0029, KL Div: 2638.2446289\n",
      "Epoch[37/50], Step [401/600], Total Loss: 12948.3916, Reconst Loss: 10229.5264, KL Div: 2718.8652344\n",
      "Epoch[37/50], Step [501/600], Total Loss: 12811.3789, Reconst Loss: 10038.3281, KL Div: 2773.0507812\n",
      "Epoch[38/50], Step [1/600], Total Loss: 12257.9199, Reconst Loss: 9568.0283, KL Div: 2689.8911133\n",
      "Epoch[38/50], Step [101/600], Total Loss: 13003.8193, Reconst Loss: 10179.3652, KL Div: 2824.4541016\n",
      "Epoch[38/50], Step [201/600], Total Loss: 13540.2061, Reconst Loss: 10700.5039, KL Div: 2839.7021484\n",
      "Epoch[38/50], Step [301/600], Total Loss: 12423.0068, Reconst Loss: 9818.5215, KL Div: 2604.4855957\n",
      "Epoch[38/50], Step [401/600], Total Loss: 12629.4658, Reconst Loss: 10018.3652, KL Div: 2611.1003418\n",
      "Epoch[38/50], Step [501/600], Total Loss: 12197.3125, Reconst Loss: 9662.4443, KL Div: 2534.8681641\n",
      "Epoch[39/50], Step [1/600], Total Loss: 12438.0986, Reconst Loss: 9799.0254, KL Div: 2639.0734863\n",
      "Epoch[39/50], Step [101/600], Total Loss: 12728.4971, Reconst Loss: 10039.7227, KL Div: 2688.7741699\n",
      "Epoch[39/50], Step [201/600], Total Loss: 12930.2441, Reconst Loss: 10252.8564, KL Div: 2677.3876953\n",
      "Epoch[39/50], Step [301/600], Total Loss: 12833.4951, Reconst Loss: 10125.5068, KL Div: 2707.9885254\n",
      "Epoch[39/50], Step [401/600], Total Loss: 12795.1230, Reconst Loss: 10246.1943, KL Div: 2548.9291992\n",
      "Epoch[39/50], Step [501/600], Total Loss: 12793.5820, Reconst Loss: 10035.6211, KL Div: 2757.9609375\n",
      "Epoch[40/50], Step [1/600], Total Loss: 13073.2773, Reconst Loss: 10265.2090, KL Div: 2808.0678711\n",
      "Epoch[40/50], Step [101/600], Total Loss: 12974.9258, Reconst Loss: 10237.9775, KL Div: 2736.9482422\n",
      "Epoch[40/50], Step [201/600], Total Loss: 13209.6484, Reconst Loss: 10473.3164, KL Div: 2736.3325195\n",
      "Epoch[40/50], Step [301/600], Total Loss: 12880.9102, Reconst Loss: 10134.5986, KL Div: 2746.3117676\n",
      "Epoch[40/50], Step [401/600], Total Loss: 12543.5127, Reconst Loss: 9843.0674, KL Div: 2700.4453125\n",
      "Epoch[40/50], Step [501/600], Total Loss: 12327.7109, Reconst Loss: 9588.3545, KL Div: 2739.3564453\n",
      "Epoch[41/50], Step [1/600], Total Loss: 12192.0977, Reconst Loss: 9600.5225, KL Div: 2591.5751953\n",
      "Epoch[41/50], Step [101/600], Total Loss: 12921.8418, Reconst Loss: 10228.2588, KL Div: 2693.5832520\n",
      "Epoch[41/50], Step [201/600], Total Loss: 12125.6270, Reconst Loss: 9612.0361, KL Div: 2513.5903320\n",
      "Epoch[41/50], Step [301/600], Total Loss: 12573.6230, Reconst Loss: 9864.6768, KL Div: 2708.9467773\n",
      "Epoch[41/50], Step [401/600], Total Loss: 12855.5820, Reconst Loss: 10246.1064, KL Div: 2609.4750977\n",
      "Epoch[41/50], Step [501/600], Total Loss: 12854.6719, Reconst Loss: 10402.3379, KL Div: 2452.3342285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[42/50], Step [1/600], Total Loss: 12351.3057, Reconst Loss: 9723.6865, KL Div: 2627.6193848\n",
      "Epoch[42/50], Step [101/600], Total Loss: 12989.3398, Reconst Loss: 10193.9453, KL Div: 2795.3950195\n",
      "Epoch[42/50], Step [201/600], Total Loss: 12400.7617, Reconst Loss: 9858.6709, KL Div: 2542.0908203\n",
      "Epoch[42/50], Step [301/600], Total Loss: 13046.2617, Reconst Loss: 10348.4268, KL Div: 2697.8354492\n",
      "Epoch[42/50], Step [401/600], Total Loss: 12212.1562, Reconst Loss: 9645.1836, KL Div: 2566.9721680\n",
      "Epoch[42/50], Step [501/600], Total Loss: 12921.2119, Reconst Loss: 10186.7949, KL Div: 2734.4167480\n",
      "Epoch[43/50], Step [1/600], Total Loss: 12406.3867, Reconst Loss: 9676.5762, KL Div: 2729.8107910\n",
      "Epoch[43/50], Step [101/600], Total Loss: 12313.7969, Reconst Loss: 9760.2305, KL Div: 2553.5666504\n",
      "Epoch[43/50], Step [201/600], Total Loss: 12760.5859, Reconst Loss: 10142.8340, KL Div: 2617.7517090\n",
      "Epoch[43/50], Step [301/600], Total Loss: 13110.9980, Reconst Loss: 10275.3096, KL Div: 2835.6884766\n",
      "Epoch[43/50], Step [401/600], Total Loss: 13766.8242, Reconst Loss: 10867.7646, KL Div: 2899.0595703\n",
      "Epoch[43/50], Step [501/600], Total Loss: 12594.8506, Reconst Loss: 9992.1123, KL Div: 2602.7385254\n",
      "Epoch[44/50], Step [1/600], Total Loss: 12452.8584, Reconst Loss: 9700.0938, KL Div: 2752.7644043\n",
      "Epoch[44/50], Step [101/600], Total Loss: 12319.7656, Reconst Loss: 9806.4561, KL Div: 2513.3098145\n",
      "Epoch[44/50], Step [201/600], Total Loss: 12404.2061, Reconst Loss: 9611.8184, KL Div: 2792.3879395\n",
      "Epoch[44/50], Step [301/600], Total Loss: 13065.3203, Reconst Loss: 10535.4951, KL Div: 2529.8249512\n",
      "Epoch[44/50], Step [401/600], Total Loss: 12880.0615, Reconst Loss: 10139.4062, KL Div: 2740.6555176\n",
      "Epoch[44/50], Step [501/600], Total Loss: 12298.4932, Reconst Loss: 9651.9756, KL Div: 2646.5173340\n",
      "Epoch[45/50], Step [1/600], Total Loss: 12802.3242, Reconst Loss: 9973.9756, KL Div: 2828.3483887\n",
      "Epoch[45/50], Step [101/600], Total Loss: 12631.0410, Reconst Loss: 9966.3457, KL Div: 2664.6953125\n",
      "Epoch[45/50], Step [201/600], Total Loss: 11968.5342, Reconst Loss: 9472.6729, KL Div: 2495.8610840\n",
      "Epoch[45/50], Step [301/600], Total Loss: 12926.8125, Reconst Loss: 10120.7695, KL Div: 2806.0434570\n",
      "Epoch[45/50], Step [401/600], Total Loss: 12255.1621, Reconst Loss: 9651.5752, KL Div: 2603.5869141\n",
      "Epoch[45/50], Step [501/600], Total Loss: 12856.7041, Reconst Loss: 10085.3477, KL Div: 2771.3562012\n",
      "Epoch[46/50], Step [1/600], Total Loss: 12697.8350, Reconst Loss: 10083.4795, KL Div: 2614.3554688\n",
      "Epoch[46/50], Step [101/600], Total Loss: 12659.0566, Reconst Loss: 9938.9551, KL Div: 2720.1010742\n",
      "Epoch[46/50], Step [201/600], Total Loss: 12623.3330, Reconst Loss: 9921.5771, KL Div: 2701.7556152\n",
      "Epoch[46/50], Step [301/600], Total Loss: 12667.7168, Reconst Loss: 9936.2207, KL Div: 2731.4960938\n",
      "Epoch[46/50], Step [401/600], Total Loss: 13121.2793, Reconst Loss: 10269.7275, KL Div: 2851.5522461\n",
      "Epoch[46/50], Step [501/600], Total Loss: 12357.1562, Reconst Loss: 9518.2354, KL Div: 2838.9211426\n",
      "Epoch[47/50], Step [1/600], Total Loss: 11730.3799, Reconst Loss: 9175.1816, KL Div: 2555.1979980\n",
      "Epoch[47/50], Step [101/600], Total Loss: 13157.1602, Reconst Loss: 10468.4053, KL Div: 2688.7546387\n",
      "Epoch[47/50], Step [201/600], Total Loss: 12788.1494, Reconst Loss: 10072.4062, KL Div: 2715.7434082\n",
      "Epoch[47/50], Step [301/600], Total Loss: 13359.5059, Reconst Loss: 10378.7939, KL Div: 2980.7114258\n",
      "Epoch[47/50], Step [401/600], Total Loss: 12984.0996, Reconst Loss: 10216.7139, KL Div: 2767.3854980\n",
      "Epoch[47/50], Step [501/600], Total Loss: 12060.4014, Reconst Loss: 9603.7793, KL Div: 2456.6220703\n",
      "Epoch[48/50], Step [1/600], Total Loss: 12451.2363, Reconst Loss: 9972.6826, KL Div: 2478.5541992\n",
      "Epoch[48/50], Step [101/600], Total Loss: 12064.9854, Reconst Loss: 9483.9014, KL Div: 2581.0837402\n",
      "Epoch[48/50], Step [201/600], Total Loss: 12531.8008, Reconst Loss: 9865.8545, KL Div: 2665.9460449\n",
      "Epoch[48/50], Step [301/600], Total Loss: 12790.4014, Reconst Loss: 10090.6709, KL Div: 2699.7304688\n",
      "Epoch[48/50], Step [401/600], Total Loss: 12358.1455, Reconst Loss: 9749.8916, KL Div: 2608.2539062\n",
      "Epoch[48/50], Step [501/600], Total Loss: 12535.5674, Reconst Loss: 9940.8447, KL Div: 2594.7229004\n",
      "Epoch[49/50], Step [1/600], Total Loss: 12341.2148, Reconst Loss: 9728.1191, KL Div: 2613.0952148\n",
      "Epoch[49/50], Step [101/600], Total Loss: 12329.1738, Reconst Loss: 9659.4199, KL Div: 2669.7534180\n",
      "Epoch[49/50], Step [201/600], Total Loss: 13221.1973, Reconst Loss: 10446.1182, KL Div: 2775.0793457\n",
      "Epoch[49/50], Step [301/600], Total Loss: 12644.5010, Reconst Loss: 9885.6455, KL Div: 2758.8557129\n",
      "Epoch[49/50], Step [401/600], Total Loss: 12904.7031, Reconst Loss: 10163.1992, KL Div: 2741.5043945\n",
      "Epoch[49/50], Step [501/600], Total Loss: 12491.8438, Reconst Loss: 9892.6221, KL Div: 2599.2216797\n",
      "Epoch[50/50], Step [1/600], Total Loss: 12488.1650, Reconst Loss: 9784.5762, KL Div: 2703.5891113\n",
      "Epoch[50/50], Step [101/600], Total Loss: 12667.2051, Reconst Loss: 10018.4541, KL Div: 2648.7504883\n",
      "Epoch[50/50], Step [201/600], Total Loss: 12568.0312, Reconst Loss: 10017.8848, KL Div: 2550.1459961\n",
      "Epoch[50/50], Step [301/600], Total Loss: 12445.3076, Reconst Loss: 9730.9961, KL Div: 2714.3115234\n",
      "Epoch[50/50], Step [401/600], Total Loss: 12493.6221, Reconst Loss: 9912.9668, KL Div: 2580.6555176\n",
      "Epoch[50/50], Step [501/600], Total Loss: 12500.3047, Reconst Loss: 10044.0029, KL Div: 2456.3012695\n"
     ]
    }
   ],
   "source": [
    "#Define model\n",
    "vae = LinearVAE()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    vae.cuda()\n",
    "    \n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
    "iter_per_epoch = len(data_loader)\n",
    "data_iter = iter(data_loader)\n",
    "\n",
    "# fixed inputs for debugging\n",
    "fixed_z = to_var(torch.randn(100, 20))\n",
    "fixed_x, _ = next(data_iter)\n",
    "torchvision.utils.save_image(fixed_x.cpu(), './data/real_images.png')\n",
    "fixed_x = to_var(fixed_x.view(fixed_x.size(0), -1))\n",
    "\n",
    "for epoch in range(50):\n",
    "    for i, (images, _) in enumerate(data_loader):\n",
    "        \n",
    "        images = to_var(images.view(images.size(0), -1))\n",
    "        out, mu, log_var = vae(images)\n",
    "        \n",
    "        # Compute reconstruction loss and kl divergence\n",
    "        reconst_loss = F.binary_cross_entropy(out, images, size_average=False)\n",
    "        kl_divergence = torch.sum(0.5 * (mu**2 + torch.exp(log_var) - log_var -1))\n",
    "        \n",
    "        # Backprop + Optimize\n",
    "        total_loss = reconst_loss + kl_divergence\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print (\"Epoch[%d/%d], Step [%d/%d], Total Loss: %.4f, \"\n",
    "                   \"Reconst Loss: %.4f, KL Div: %.7f\" \n",
    "                   %(epoch+1, 50, i+1, iter_per_epoch, total_loss.data[0], \n",
    "                     reconst_loss.data[0], kl_divergence.data[0]))\n",
    "    \n",
    "    # Save the reconstructed images\n",
    "    reconst_images, _, _ = vae(fixed_x)\n",
    "    reconst_images = reconst_images.view(reconst_images.size(0), 1, 28, 28)\n",
    "    torchvision.utils.save_image(reconst_images.data.cpu(), \n",
    "        './data/reconst_images_%d.png' %(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design conv model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_var(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n",
    "\n",
    "# VAE model\n",
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self, image_size=12*12*16, h_dim=400, z_dim=20):\n",
    "        super(ConvVAE, self).__init__()\n",
    "        self.linear1 = nn.Linear(image_size, h_dim)\n",
    "        self.linear2 = nn.Linear(h_dim, z_dim)\n",
    "        self.linear3 = nn.Linear(z_dim, h_dim)\n",
    "        self.linear4 = nn.Linear(h_dim, 16*8*8)\n",
    "        self.batch_norm = nn.BatchNorm2d(16)\n",
    "        self.conv2D1 = nn.Conv2d(1, 16, 5)\n",
    "        self.conv2D2 = nn.Conv2d(16, 16, 5)\n",
    "        self.trans_conv2D_1 = nn.ConvTranspose2d(16, 16, 5)\n",
    "        self.trans_conv2D_2 = nn.ConvTranspose2d(16, 1, 5)\n",
    "    \n",
    "    def encoder(self, x): #(batch, 28, 28, 1)\n",
    "        x_conv = self.batch_norm(F.relu(self.conv2D1(x))) #(batch, 24, 24, 16)\n",
    "        x_conv = self.batch_norm(F.relu(self.conv2D2(x_conv))) #(batch, 20, 20, 16)\n",
    "        x_conv = self.batch_norm(F.relu(self.conv2D2(x_conv))) #(batch, 16, 16, 16)\n",
    "        x_conv = self.batch_norm(F.relu(self.conv2D2(x_conv))) #(batch, 12, 12, 16)\n",
    "        x_conv = F.relu(self.linear1(x_conv.view(-1, 12*12*16))) #(batch, 12*12*16) -- > #(batch, 400)\n",
    "        z_mean = self.linear2(x_conv) #(batch, 20)\n",
    "        z_log_var = self.linear2(x_conv) #(batch, 20)\n",
    "        return z_mean, z_log_var\n",
    "    \n",
    "    def decoder(self, z):\n",
    "        z = F.relu(self.linear3(z))\n",
    "        z = F.relu(self.linear4(z))\n",
    "        recon_x = z.resize(z.size(0), 16, 8, 8)\n",
    "        recon_x = self.batch_norm(F.relu(self.trans_conv2D_1(recon_x)))\n",
    "        recon_x = self.batch_norm(F.relu(self.trans_conv2D_1(recon_x)))\n",
    "        recon_x = self.batch_norm(F.relu(self.trans_conv2D_1(recon_x)))\n",
    "        recon_x = self.batch_norm(F.relu(self.trans_conv2D_1(recon_x)))\n",
    "        recon_x = F.sigmoid(self.trans_conv2D_2(recon_x))\n",
    "        return recon_x\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\"z = mean + eps * sigma where eps is sampled from N(0, 1).\"\"\"\n",
    "        eps = to_var(torch.randn(mu.size(0), mu.size(1)))\n",
    "        z = mu + eps * torch.exp(log_var/2)    # 2 for convert var to std\n",
    "        return z\n",
    "                     \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x) # torch.chunk(h, 2, dim=1)  # mean and log variance.\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        out = self.decoder(z)\n",
    "        return out, mu, log_var\n",
    "    \n",
    "    def sample(self, z):\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train conv model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/50], Step [1/600], Total Loss: 54833.7773, Reconst Loss: 54814.1133, KL Div: 19.6647511\n",
      "Epoch[1/50], Step [101/600], Total Loss: 19597.5410, Reconst Loss: 18214.1035, KL Div: 1383.4370117\n",
      "Epoch[1/50], Step [201/600], Total Loss: 15769.4365, Reconst Loss: 13937.5801, KL Div: 1831.8562012\n",
      "Epoch[1/50], Step [301/600], Total Loss: 15296.2949, Reconst Loss: 13054.0576, KL Div: 2242.2368164\n",
      "Epoch[1/50], Step [401/600], Total Loss: 15303.2285, Reconst Loss: 12995.6611, KL Div: 2307.5671387\n",
      "Epoch[1/50], Step [501/600], Total Loss: 14686.3906, Reconst Loss: 12294.1777, KL Div: 2392.2126465\n",
      "Epoch[2/50], Step [1/600], Total Loss: 14962.1816, Reconst Loss: 12686.5479, KL Div: 2275.6340332\n",
      "Epoch[2/50], Step [101/600], Total Loss: 14555.4727, Reconst Loss: 12280.4082, KL Div: 2275.0642090\n",
      "Epoch[2/50], Step [201/600], Total Loss: 13933.2549, Reconst Loss: 11559.5010, KL Div: 2373.7536621\n",
      "Epoch[2/50], Step [301/600], Total Loss: 13660.2422, Reconst Loss: 11308.8398, KL Div: 2351.4025879\n",
      "Epoch[2/50], Step [401/600], Total Loss: 14311.2949, Reconst Loss: 11972.9912, KL Div: 2338.3039551\n",
      "Epoch[2/50], Step [501/600], Total Loss: 14559.4199, Reconst Loss: 12134.8115, KL Div: 2424.6086426\n",
      "Epoch[3/50], Step [1/600], Total Loss: 14413.6504, Reconst Loss: 12103.1338, KL Div: 2310.5166016\n",
      "Epoch[3/50], Step [101/600], Total Loss: 13787.9375, Reconst Loss: 11333.7314, KL Div: 2454.2062988\n",
      "Epoch[3/50], Step [201/600], Total Loss: 14094.4043, Reconst Loss: 11580.0732, KL Div: 2514.3308105\n",
      "Epoch[3/50], Step [301/600], Total Loss: 13515.9785, Reconst Loss: 11198.7021, KL Div: 2317.2768555\n",
      "Epoch[3/50], Step [401/600], Total Loss: 13819.1641, Reconst Loss: 11353.0723, KL Div: 2466.0920410\n",
      "Epoch[3/50], Step [501/600], Total Loss: 13164.1445, Reconst Loss: 10730.2139, KL Div: 2433.9306641\n",
      "Epoch[4/50], Step [1/600], Total Loss: 13763.1660, Reconst Loss: 11247.7861, KL Div: 2515.3798828\n",
      "Epoch[4/50], Step [101/600], Total Loss: 14035.2480, Reconst Loss: 11645.5107, KL Div: 2389.7377930\n",
      "Epoch[4/50], Step [201/600], Total Loss: 13606.7715, Reconst Loss: 11179.6172, KL Div: 2427.1540527\n",
      "Epoch[4/50], Step [301/600], Total Loss: 13330.0078, Reconst Loss: 10873.9512, KL Div: 2456.0566406\n",
      "Epoch[4/50], Step [401/600], Total Loss: 13510.9219, Reconst Loss: 11161.2441, KL Div: 2349.6779785\n",
      "Epoch[4/50], Step [501/600], Total Loss: 13713.9980, Reconst Loss: 11182.9736, KL Div: 2531.0239258\n",
      "Epoch[5/50], Step [1/600], Total Loss: 13289.8535, Reconst Loss: 10811.4355, KL Div: 2478.4184570\n",
      "Epoch[5/50], Step [101/600], Total Loss: 13519.2910, Reconst Loss: 11001.0459, KL Div: 2518.2453613\n",
      "Epoch[5/50], Step [201/600], Total Loss: 13772.5352, Reconst Loss: 11155.5977, KL Div: 2616.9377441\n",
      "Epoch[5/50], Step [301/600], Total Loss: 13070.4102, Reconst Loss: 10411.5391, KL Div: 2658.8710938\n",
      "Epoch[5/50], Step [401/600], Total Loss: 13569.9443, Reconst Loss: 11040.3994, KL Div: 2529.5451660\n",
      "Epoch[5/50], Step [501/600], Total Loss: 13785.3066, Reconst Loss: 11295.6592, KL Div: 2489.6472168\n",
      "Epoch[6/50], Step [1/600], Total Loss: 13235.0332, Reconst Loss: 10822.7354, KL Div: 2412.2973633\n",
      "Epoch[6/50], Step [101/600], Total Loss: 13073.9697, Reconst Loss: 10590.1279, KL Div: 2483.8420410\n",
      "Epoch[6/50], Step [201/600], Total Loss: 12598.5732, Reconst Loss: 10179.6035, KL Div: 2418.9697266\n",
      "Epoch[6/50], Step [301/600], Total Loss: 12610.1533, Reconst Loss: 10185.8926, KL Div: 2424.2607422\n",
      "Epoch[6/50], Step [401/600], Total Loss: 13591.8037, Reconst Loss: 11087.9170, KL Div: 2503.8867188\n",
      "Epoch[6/50], Step [501/600], Total Loss: 12998.6787, Reconst Loss: 10590.0059, KL Div: 2408.6728516\n",
      "Epoch[7/50], Step [1/600], Total Loss: 13134.4658, Reconst Loss: 10593.1240, KL Div: 2541.3417969\n",
      "Epoch[7/50], Step [101/600], Total Loss: 13217.8203, Reconst Loss: 10786.9893, KL Div: 2430.8310547\n",
      "Epoch[7/50], Step [201/600], Total Loss: 13668.1602, Reconst Loss: 11073.8203, KL Div: 2594.3400879\n",
      "Epoch[7/50], Step [301/600], Total Loss: 12606.4570, Reconst Loss: 10246.0762, KL Div: 2360.3803711\n",
      "Epoch[7/50], Step [401/600], Total Loss: 13070.2598, Reconst Loss: 10564.1992, KL Div: 2506.0610352\n",
      "Epoch[7/50], Step [501/600], Total Loss: 13170.4297, Reconst Loss: 10574.9111, KL Div: 2595.5180664\n",
      "Epoch[8/50], Step [1/600], Total Loss: 13291.3076, Reconst Loss: 10703.4775, KL Div: 2587.8303223\n",
      "Epoch[8/50], Step [101/600], Total Loss: 13340.0713, Reconst Loss: 10923.6055, KL Div: 2416.4658203\n",
      "Epoch[8/50], Step [201/600], Total Loss: 12688.8301, Reconst Loss: 10172.2598, KL Div: 2516.5703125\n",
      "Epoch[8/50], Step [301/600], Total Loss: 13048.1777, Reconst Loss: 10466.5889, KL Div: 2581.5883789\n",
      "Epoch[8/50], Step [401/600], Total Loss: 13003.8594, Reconst Loss: 10467.0645, KL Div: 2536.7951660\n",
      "Epoch[8/50], Step [501/600], Total Loss: 12793.2686, Reconst Loss: 10230.6504, KL Div: 2562.6181641\n",
      "Epoch[9/50], Step [1/600], Total Loss: 12695.1982, Reconst Loss: 10220.3984, KL Div: 2474.7998047\n",
      "Epoch[9/50], Step [101/600], Total Loss: 13136.3115, Reconst Loss: 10773.2119, KL Div: 2363.0998535\n",
      "Epoch[9/50], Step [201/600], Total Loss: 12831.1152, Reconst Loss: 10285.3496, KL Div: 2545.7661133\n",
      "Epoch[9/50], Step [301/600], Total Loss: 12967.0352, Reconst Loss: 10458.1006, KL Div: 2508.9348145\n",
      "Epoch[9/50], Step [401/600], Total Loss: 13002.4316, Reconst Loss: 10588.7656, KL Div: 2413.6657715\n",
      "Epoch[9/50], Step [501/600], Total Loss: 13481.4844, Reconst Loss: 10808.7129, KL Div: 2672.7709961\n",
      "Epoch[10/50], Step [1/600], Total Loss: 12852.7100, Reconst Loss: 10275.0000, KL Div: 2577.7099609\n",
      "Epoch[10/50], Step [101/600], Total Loss: 12880.2373, Reconst Loss: 10281.8008, KL Div: 2598.4367676\n",
      "Epoch[10/50], Step [201/600], Total Loss: 12565.1406, Reconst Loss: 10095.6475, KL Div: 2469.4926758\n",
      "Epoch[10/50], Step [301/600], Total Loss: 12651.0049, Reconst Loss: 10175.3203, KL Div: 2475.6848145\n",
      "Epoch[10/50], Step [401/600], Total Loss: 12594.5088, Reconst Loss: 10104.8418, KL Div: 2489.6672363\n",
      "Epoch[10/50], Step [501/600], Total Loss: 13152.8682, Reconst Loss: 10553.7959, KL Div: 2599.0725098\n",
      "Epoch[11/50], Step [1/600], Total Loss: 12319.1514, Reconst Loss: 9859.6494, KL Div: 2459.5021973\n",
      "Epoch[11/50], Step [101/600], Total Loss: 13220.6357, Reconst Loss: 10670.4121, KL Div: 2550.2236328\n",
      "Epoch[11/50], Step [201/600], Total Loss: 12302.3945, Reconst Loss: 9985.9248, KL Div: 2316.4692383\n",
      "Epoch[11/50], Step [301/600], Total Loss: 13239.7012, Reconst Loss: 10617.9727, KL Div: 2621.7280273\n",
      "Epoch[11/50], Step [401/600], Total Loss: 13150.2617, Reconst Loss: 10728.0068, KL Div: 2422.2546387\n",
      "Epoch[11/50], Step [501/600], Total Loss: 12481.3525, Reconst Loss: 10060.8604, KL Div: 2420.4921875\n",
      "Epoch[12/50], Step [1/600], Total Loss: 13216.6338, Reconst Loss: 10649.9424, KL Div: 2566.6914062\n",
      "Epoch[12/50], Step [101/600], Total Loss: 13036.3594, Reconst Loss: 10418.0615, KL Div: 2618.2983398\n",
      "Epoch[12/50], Step [201/600], Total Loss: 12761.5137, Reconst Loss: 10150.2939, KL Div: 2611.2202148\n",
      "Epoch[12/50], Step [301/600], Total Loss: 12638.7363, Reconst Loss: 10236.9961, KL Div: 2401.7404785\n",
      "Epoch[12/50], Step [401/600], Total Loss: 12743.4678, Reconst Loss: 10163.6738, KL Div: 2579.7939453\n",
      "Epoch[12/50], Step [501/600], Total Loss: 13683.6875, Reconst Loss: 11080.0010, KL Div: 2603.6870117\n",
      "Epoch[13/50], Step [1/600], Total Loss: 12859.9854, Reconst Loss: 10195.6191, KL Div: 2664.3662109\n",
      "Epoch[13/50], Step [101/600], Total Loss: 12442.3857, Reconst Loss: 9859.8301, KL Div: 2582.5556641\n",
      "Epoch[13/50], Step [201/600], Total Loss: 12934.0645, Reconst Loss: 10354.5527, KL Div: 2579.5119629\n",
      "Epoch[13/50], Step [301/600], Total Loss: 12414.2891, Reconst Loss: 9864.9795, KL Div: 2549.3098145\n",
      "Epoch[13/50], Step [401/600], Total Loss: 13126.7461, Reconst Loss: 10569.5176, KL Div: 2557.2282715\n",
      "Epoch[13/50], Step [501/600], Total Loss: 12932.7695, Reconst Loss: 10545.6846, KL Div: 2387.0852051\n",
      "Epoch[14/50], Step [1/600], Total Loss: 12348.5137, Reconst Loss: 9786.3262, KL Div: 2562.1870117\n",
      "Epoch[14/50], Step [101/600], Total Loss: 12992.7324, Reconst Loss: 10432.0430, KL Div: 2560.6894531\n",
      "Epoch[14/50], Step [201/600], Total Loss: 12477.1924, Reconst Loss: 9963.1016, KL Div: 2514.0908203\n",
      "Epoch[14/50], Step [301/600], Total Loss: 12198.7461, Reconst Loss: 9632.5029, KL Div: 2566.2426758\n",
      "Epoch[14/50], Step [401/600], Total Loss: 12784.0303, Reconst Loss: 10272.9590, KL Div: 2511.0715332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[14/50], Step [501/600], Total Loss: 12595.4951, Reconst Loss: 10182.8066, KL Div: 2412.6884766\n",
      "Epoch[15/50], Step [1/600], Total Loss: 12977.0127, Reconst Loss: 10382.8408, KL Div: 2594.1721191\n",
      "Epoch[15/50], Step [101/600], Total Loss: 12452.5986, Reconst Loss: 10004.2793, KL Div: 2448.3193359\n",
      "Epoch[15/50], Step [201/600], Total Loss: 12509.1504, Reconst Loss: 9973.4902, KL Div: 2535.6601562\n",
      "Epoch[15/50], Step [301/600], Total Loss: 12219.8555, Reconst Loss: 9708.3311, KL Div: 2511.5249023\n",
      "Epoch[15/50], Step [401/600], Total Loss: 12611.0332, Reconst Loss: 10042.5293, KL Div: 2568.5041504\n",
      "Epoch[15/50], Step [501/600], Total Loss: 12921.8936, Reconst Loss: 10416.3389, KL Div: 2505.5546875\n",
      "Epoch[16/50], Step [1/600], Total Loss: 12675.0723, Reconst Loss: 10079.0371, KL Div: 2596.0356445\n",
      "Epoch[16/50], Step [101/600], Total Loss: 12374.3887, Reconst Loss: 9881.1738, KL Div: 2493.2150879\n",
      "Epoch[16/50], Step [201/600], Total Loss: 12270.0254, Reconst Loss: 9724.1709, KL Div: 2545.8544922\n",
      "Epoch[16/50], Step [301/600], Total Loss: 12698.3066, Reconst Loss: 10116.8701, KL Div: 2581.4370117\n",
      "Epoch[16/50], Step [401/600], Total Loss: 12685.2207, Reconst Loss: 10105.6201, KL Div: 2579.6003418\n",
      "Epoch[16/50], Step [501/600], Total Loss: 13159.3867, Reconst Loss: 10547.4824, KL Div: 2611.9040527\n",
      "Epoch[17/50], Step [1/600], Total Loss: 12887.5957, Reconst Loss: 10301.1973, KL Div: 2586.3981934\n",
      "Epoch[17/50], Step [101/600], Total Loss: 12351.9238, Reconst Loss: 9912.7148, KL Div: 2439.2092285\n",
      "Epoch[17/50], Step [201/600], Total Loss: 12553.9346, Reconst Loss: 9968.8828, KL Div: 2585.0520020\n",
      "Epoch[17/50], Step [301/600], Total Loss: 12333.7773, Reconst Loss: 9798.7529, KL Div: 2535.0246582\n",
      "Epoch[17/50], Step [401/600], Total Loss: 12629.7773, Reconst Loss: 10015.9551, KL Div: 2613.8220215\n",
      "Epoch[17/50], Step [501/600], Total Loss: 12461.7148, Reconst Loss: 9903.8389, KL Div: 2557.8757324\n",
      "Epoch[18/50], Step [1/600], Total Loss: 13316.8018, Reconst Loss: 10691.6035, KL Div: 2625.1982422\n",
      "Epoch[18/50], Step [101/600], Total Loss: 12322.3145, Reconst Loss: 9783.8281, KL Div: 2538.4860840\n",
      "Epoch[18/50], Step [201/600], Total Loss: 12437.5410, Reconst Loss: 9863.6318, KL Div: 2573.9094238\n",
      "Epoch[18/50], Step [301/600], Total Loss: 13232.1182, Reconst Loss: 10674.2412, KL Div: 2557.8771973\n",
      "Epoch[18/50], Step [401/600], Total Loss: 12723.7998, Reconst Loss: 10163.2793, KL Div: 2560.5205078\n",
      "Epoch[18/50], Step [501/600], Total Loss: 12418.7988, Reconst Loss: 9845.8896, KL Div: 2572.9094238\n",
      "Epoch[19/50], Step [1/600], Total Loss: 12735.4258, Reconst Loss: 10149.5410, KL Div: 2585.8845215\n",
      "Epoch[19/50], Step [101/600], Total Loss: 12519.8438, Reconst Loss: 10092.4209, KL Div: 2427.4233398\n",
      "Epoch[19/50], Step [201/600], Total Loss: 12631.6348, Reconst Loss: 9942.7930, KL Div: 2688.8415527\n",
      "Epoch[19/50], Step [301/600], Total Loss: 12399.8652, Reconst Loss: 9948.8662, KL Div: 2450.9992676\n",
      "Epoch[19/50], Step [401/600], Total Loss: 12730.6953, Reconst Loss: 10143.5020, KL Div: 2587.1938477\n",
      "Epoch[19/50], Step [501/600], Total Loss: 13079.5674, Reconst Loss: 10461.5195, KL Div: 2618.0476074\n",
      "Epoch[20/50], Step [1/600], Total Loss: 12875.2070, Reconst Loss: 10285.8467, KL Div: 2589.3601074\n",
      "Epoch[20/50], Step [101/600], Total Loss: 13220.3457, Reconst Loss: 10646.2666, KL Div: 2574.0788574\n",
      "Epoch[20/50], Step [201/600], Total Loss: 12280.5469, Reconst Loss: 9737.8281, KL Div: 2542.7185059\n",
      "Epoch[20/50], Step [301/600], Total Loss: 12642.7266, Reconst Loss: 10037.9170, KL Div: 2604.8100586\n",
      "Epoch[20/50], Step [401/600], Total Loss: 12725.2246, Reconst Loss: 10173.5166, KL Div: 2551.7084961\n",
      "Epoch[20/50], Step [501/600], Total Loss: 12788.6719, Reconst Loss: 10223.2314, KL Div: 2565.4406738\n",
      "Epoch[21/50], Step [1/600], Total Loss: 12867.8301, Reconst Loss: 10373.5264, KL Div: 2494.3037109\n",
      "Epoch[21/50], Step [101/600], Total Loss: 12674.2070, Reconst Loss: 10036.4580, KL Div: 2637.7490234\n",
      "Epoch[21/50], Step [201/600], Total Loss: 12084.3652, Reconst Loss: 9550.1855, KL Div: 2534.1799316\n",
      "Epoch[21/50], Step [301/600], Total Loss: 12447.8145, Reconst Loss: 9868.5898, KL Div: 2579.2250977\n",
      "Epoch[21/50], Step [401/600], Total Loss: 12778.9111, Reconst Loss: 10238.2197, KL Div: 2540.6916504\n",
      "Epoch[21/50], Step [501/600], Total Loss: 12722.1680, Reconst Loss: 10079.8223, KL Div: 2642.3461914\n",
      "Epoch[22/50], Step [1/600], Total Loss: 12782.1699, Reconst Loss: 10209.1328, KL Div: 2573.0375977\n",
      "Epoch[22/50], Step [101/600], Total Loss: 12851.7988, Reconst Loss: 10243.7900, KL Div: 2608.0085449\n",
      "Epoch[22/50], Step [201/600], Total Loss: 12589.2910, Reconst Loss: 9996.0869, KL Div: 2593.2036133\n",
      "Epoch[22/50], Step [301/600], Total Loss: 12590.1973, Reconst Loss: 9935.8184, KL Div: 2654.3789062\n",
      "Epoch[22/50], Step [401/600], Total Loss: 12838.6631, Reconst Loss: 10216.8379, KL Div: 2621.8251953\n",
      "Epoch[22/50], Step [501/600], Total Loss: 12306.3320, Reconst Loss: 9873.4443, KL Div: 2432.8872070\n",
      "Epoch[23/50], Step [1/600], Total Loss: 12218.0850, Reconst Loss: 9722.8857, KL Div: 2495.1992188\n",
      "Epoch[23/50], Step [101/600], Total Loss: 12831.9805, Reconst Loss: 10212.1074, KL Div: 2619.8730469\n",
      "Epoch[23/50], Step [201/600], Total Loss: 12357.2266, Reconst Loss: 9797.1582, KL Div: 2560.0681152\n",
      "Epoch[23/50], Step [301/600], Total Loss: 12395.6592, Reconst Loss: 9788.4248, KL Div: 2607.2343750\n",
      "Epoch[23/50], Step [401/600], Total Loss: 12140.5156, Reconst Loss: 9571.6299, KL Div: 2568.8852539\n",
      "Epoch[23/50], Step [501/600], Total Loss: 12298.5518, Reconst Loss: 9620.6250, KL Div: 2677.9267578\n",
      "Epoch[24/50], Step [1/600], Total Loss: 12162.0225, Reconst Loss: 9758.5713, KL Div: 2403.4509277\n",
      "Epoch[24/50], Step [101/600], Total Loss: 11719.7559, Reconst Loss: 9276.5420, KL Div: 2443.2138672\n",
      "Epoch[24/50], Step [201/600], Total Loss: 13063.0830, Reconst Loss: 10409.2227, KL Div: 2653.8603516\n",
      "Epoch[24/50], Step [301/600], Total Loss: 13268.6064, Reconst Loss: 10582.8643, KL Div: 2685.7419434\n",
      "Epoch[24/50], Step [401/600], Total Loss: 12773.3115, Reconst Loss: 10186.5840, KL Div: 2586.7272949\n",
      "Epoch[24/50], Step [501/600], Total Loss: 12757.5586, Reconst Loss: 10056.9834, KL Div: 2700.5756836\n",
      "Epoch[25/50], Step [1/600], Total Loss: 12431.9150, Reconst Loss: 9778.5312, KL Div: 2653.3837891\n",
      "Epoch[25/50], Step [101/600], Total Loss: 12111.3252, Reconst Loss: 9522.9131, KL Div: 2588.4123535\n",
      "Epoch[25/50], Step [201/600], Total Loss: 11998.1660, Reconst Loss: 9521.1553, KL Div: 2477.0109863\n",
      "Epoch[25/50], Step [301/600], Total Loss: 12907.9316, Reconst Loss: 10270.3086, KL Div: 2637.6235352\n",
      "Epoch[25/50], Step [401/600], Total Loss: 12711.9590, Reconst Loss: 9974.1240, KL Div: 2737.8347168\n",
      "Epoch[25/50], Step [501/600], Total Loss: 13234.2891, Reconst Loss: 10434.9316, KL Div: 2799.3569336\n",
      "Epoch[26/50], Step [1/600], Total Loss: 12200.2051, Reconst Loss: 9639.4707, KL Div: 2560.7348633\n",
      "Epoch[26/50], Step [101/600], Total Loss: 12070.1484, Reconst Loss: 9569.6172, KL Div: 2500.5310059\n",
      "Epoch[26/50], Step [201/600], Total Loss: 12024.4766, Reconst Loss: 9448.0312, KL Div: 2576.4450684\n",
      "Epoch[26/50], Step [301/600], Total Loss: 13192.7891, Reconst Loss: 10537.1094, KL Div: 2655.6791992\n",
      "Epoch[26/50], Step [401/600], Total Loss: 12395.6602, Reconst Loss: 9772.5020, KL Div: 2623.1577148\n",
      "Epoch[26/50], Step [501/600], Total Loss: 12098.8672, Reconst Loss: 9559.1270, KL Div: 2539.7404785\n",
      "Epoch[27/50], Step [1/600], Total Loss: 12881.4893, Reconst Loss: 10051.6484, KL Div: 2829.8410645\n",
      "Epoch[27/50], Step [101/600], Total Loss: 11764.3408, Reconst Loss: 9219.0000, KL Div: 2545.3405762\n",
      "Epoch[27/50], Step [201/600], Total Loss: 12582.0215, Reconst Loss: 10000.4502, KL Div: 2581.5715332\n",
      "Epoch[27/50], Step [301/600], Total Loss: 12677.5752, Reconst Loss: 10065.9385, KL Div: 2611.6369629\n",
      "Epoch[27/50], Step [401/600], Total Loss: 12584.8320, Reconst Loss: 10010.0371, KL Div: 2574.7944336\n",
      "Epoch[27/50], Step [501/600], Total Loss: 12406.9512, Reconst Loss: 9809.1309, KL Div: 2597.8200684\n",
      "Epoch[28/50], Step [1/600], Total Loss: 12333.3896, Reconst Loss: 9827.8613, KL Div: 2505.5283203\n",
      "Epoch[28/50], Step [101/600], Total Loss: 12347.2383, Reconst Loss: 9688.3379, KL Div: 2658.9001465\n",
      "Epoch[28/50], Step [201/600], Total Loss: 12983.0781, Reconst Loss: 10356.7021, KL Div: 2626.3762207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[28/50], Step [301/600], Total Loss: 11750.4277, Reconst Loss: 9181.6914, KL Div: 2568.7365723\n",
      "Epoch[28/50], Step [401/600], Total Loss: 12558.6416, Reconst Loss: 9988.2910, KL Div: 2570.3505859\n",
      "Epoch[28/50], Step [501/600], Total Loss: 12728.8594, Reconst Loss: 10028.6777, KL Div: 2700.1816406\n",
      "Epoch[29/50], Step [1/600], Total Loss: 12700.5273, Reconst Loss: 10100.7939, KL Div: 2599.7333984\n",
      "Epoch[29/50], Step [101/600], Total Loss: 12655.4658, Reconst Loss: 10058.7598, KL Div: 2596.7062988\n",
      "Epoch[29/50], Step [201/600], Total Loss: 12530.5781, Reconst Loss: 9836.0400, KL Div: 2694.5385742\n",
      "Epoch[29/50], Step [301/600], Total Loss: 12462.0684, Reconst Loss: 9888.8564, KL Div: 2573.2119141\n",
      "Epoch[29/50], Step [401/600], Total Loss: 12366.0469, Reconst Loss: 9808.0801, KL Div: 2557.9663086\n",
      "Epoch[29/50], Step [501/600], Total Loss: 12365.9668, Reconst Loss: 9683.7119, KL Div: 2682.2543945\n",
      "Epoch[30/50], Step [1/600], Total Loss: 11657.2686, Reconst Loss: 9065.0967, KL Div: 2592.1716309\n",
      "Epoch[30/50], Step [101/600], Total Loss: 11920.2188, Reconst Loss: 9305.5127, KL Div: 2614.7055664\n",
      "Epoch[30/50], Step [201/600], Total Loss: 12032.9951, Reconst Loss: 9535.5205, KL Div: 2497.4743652\n",
      "Epoch[30/50], Step [301/600], Total Loss: 12288.0117, Reconst Loss: 9624.0801, KL Div: 2663.9316406\n",
      "Epoch[30/50], Step [401/600], Total Loss: 12335.6631, Reconst Loss: 9766.9824, KL Div: 2568.6806641\n",
      "Epoch[30/50], Step [501/600], Total Loss: 12532.7598, Reconst Loss: 9816.1953, KL Div: 2716.5639648\n",
      "Epoch[31/50], Step [1/600], Total Loss: 12558.6787, Reconst Loss: 9983.0703, KL Div: 2575.6083984\n",
      "Epoch[31/50], Step [101/600], Total Loss: 12536.1045, Reconst Loss: 9927.9414, KL Div: 2608.1628418\n",
      "Epoch[31/50], Step [201/600], Total Loss: 12238.2637, Reconst Loss: 9655.7842, KL Div: 2582.4790039\n",
      "Epoch[31/50], Step [301/600], Total Loss: 12473.7432, Reconst Loss: 9879.0576, KL Div: 2594.6855469\n",
      "Epoch[31/50], Step [401/600], Total Loss: 12877.3018, Reconst Loss: 10235.5967, KL Div: 2641.7048340\n",
      "Epoch[31/50], Step [501/600], Total Loss: 13121.7822, Reconst Loss: 10423.8594, KL Div: 2697.9226074\n",
      "Epoch[32/50], Step [1/600], Total Loss: 12050.3486, Reconst Loss: 9554.0107, KL Div: 2496.3381348\n",
      "Epoch[32/50], Step [101/600], Total Loss: 11680.6992, Reconst Loss: 9130.3848, KL Div: 2550.3142090\n",
      "Epoch[32/50], Step [201/600], Total Loss: 12458.0635, Reconst Loss: 9887.6895, KL Div: 2570.3740234\n",
      "Epoch[32/50], Step [301/600], Total Loss: 12518.1572, Reconst Loss: 9857.4287, KL Div: 2660.7285156\n",
      "Epoch[32/50], Step [401/600], Total Loss: 12266.6328, Reconst Loss: 9724.9287, KL Div: 2541.7043457\n",
      "Epoch[32/50], Step [501/600], Total Loss: 12328.5781, Reconst Loss: 9767.8105, KL Div: 2560.7670898\n",
      "Epoch[33/50], Step [1/600], Total Loss: 12615.9668, Reconst Loss: 10068.9219, KL Div: 2547.0446777\n",
      "Epoch[33/50], Step [101/600], Total Loss: 12046.8682, Reconst Loss: 9479.5117, KL Div: 2567.3564453\n",
      "Epoch[33/50], Step [201/600], Total Loss: 12481.6836, Reconst Loss: 9911.9707, KL Div: 2569.7131348\n",
      "Epoch[33/50], Step [301/600], Total Loss: 12093.4717, Reconst Loss: 9567.5059, KL Div: 2525.9658203\n",
      "Epoch[33/50], Step [401/600], Total Loss: 11603.6494, Reconst Loss: 9035.7939, KL Div: 2567.8554688\n",
      "Epoch[33/50], Step [501/600], Total Loss: 11903.4424, Reconst Loss: 9362.5850, KL Div: 2540.8574219\n",
      "Epoch[34/50], Step [1/600], Total Loss: 13023.6055, Reconst Loss: 10332.5957, KL Div: 2691.0095215\n",
      "Epoch[34/50], Step [101/600], Total Loss: 12820.2539, Reconst Loss: 10197.3633, KL Div: 2622.8901367\n",
      "Epoch[34/50], Step [201/600], Total Loss: 13086.7568, Reconst Loss: 10379.5586, KL Div: 2707.1979980\n",
      "Epoch[34/50], Step [301/600], Total Loss: 11943.8086, Reconst Loss: 9378.8564, KL Div: 2564.9516602\n",
      "Epoch[34/50], Step [401/600], Total Loss: 12731.1641, Reconst Loss: 9991.6768, KL Div: 2739.4875488\n",
      "Epoch[34/50], Step [501/600], Total Loss: 12690.4111, Reconst Loss: 9989.3730, KL Div: 2701.0378418\n",
      "Epoch[35/50], Step [1/600], Total Loss: 12524.3447, Reconst Loss: 9935.8799, KL Div: 2588.4648438\n",
      "Epoch[35/50], Step [101/600], Total Loss: 12616.6328, Reconst Loss: 9932.3760, KL Div: 2684.2565918\n",
      "Epoch[35/50], Step [201/600], Total Loss: 12471.9580, Reconst Loss: 9878.2822, KL Div: 2593.6760254\n",
      "Epoch[35/50], Step [301/600], Total Loss: 12592.3301, Reconst Loss: 9959.7695, KL Div: 2632.5607910\n",
      "Epoch[35/50], Step [401/600], Total Loss: 12134.8467, Reconst Loss: 9425.8379, KL Div: 2709.0087891\n",
      "Epoch[35/50], Step [501/600], Total Loss: 12945.5498, Reconst Loss: 10168.4238, KL Div: 2777.1257324\n",
      "Epoch[36/50], Step [1/600], Total Loss: 12147.6865, Reconst Loss: 9509.5947, KL Div: 2638.0920410\n",
      "Epoch[36/50], Step [101/600], Total Loss: 12244.4121, Reconst Loss: 9599.8838, KL Div: 2644.5278320\n",
      "Epoch[36/50], Step [201/600], Total Loss: 12095.3926, Reconst Loss: 9549.7432, KL Div: 2545.6489258\n",
      "Epoch[36/50], Step [301/600], Total Loss: 13040.4502, Reconst Loss: 10382.4111, KL Div: 2658.0388184\n",
      "Epoch[36/50], Step [401/600], Total Loss: 12427.6016, Reconst Loss: 9781.0010, KL Div: 2646.6003418\n",
      "Epoch[36/50], Step [501/600], Total Loss: 12110.6416, Reconst Loss: 9579.8975, KL Div: 2530.7438965\n",
      "Epoch[37/50], Step [1/600], Total Loss: 12988.0977, Reconst Loss: 10230.3213, KL Div: 2757.7768555\n",
      "Epoch[37/50], Step [101/600], Total Loss: 12250.8154, Reconst Loss: 9623.1621, KL Div: 2627.6533203\n",
      "Epoch[37/50], Step [201/600], Total Loss: 12452.0918, Reconst Loss: 9675.8057, KL Div: 2776.2856445\n",
      "Epoch[37/50], Step [301/600], Total Loss: 13223.6250, Reconst Loss: 10441.0234, KL Div: 2782.6018066\n",
      "Epoch[37/50], Step [401/600], Total Loss: 12373.3799, Reconst Loss: 9787.8066, KL Div: 2585.5729980\n",
      "Epoch[37/50], Step [501/600], Total Loss: 12008.6895, Reconst Loss: 9401.8936, KL Div: 2606.7954102\n",
      "Epoch[38/50], Step [1/600], Total Loss: 11960.8750, Reconst Loss: 9391.7129, KL Div: 2569.1625977\n",
      "Epoch[38/50], Step [101/600], Total Loss: 12346.0732, Reconst Loss: 9676.5000, KL Div: 2669.5729980\n",
      "Epoch[38/50], Step [201/600], Total Loss: 12393.9248, Reconst Loss: 9787.7168, KL Div: 2606.2077637\n",
      "Epoch[38/50], Step [301/600], Total Loss: 11798.4688, Reconst Loss: 9328.7969, KL Div: 2469.6713867\n",
      "Epoch[38/50], Step [401/600], Total Loss: 12110.5117, Reconst Loss: 9423.7354, KL Div: 2686.7758789\n",
      "Epoch[38/50], Step [501/600], Total Loss: 12281.6377, Reconst Loss: 9707.7969, KL Div: 2573.8405762\n",
      "Epoch[39/50], Step [1/600], Total Loss: 12363.6680, Reconst Loss: 9685.5508, KL Div: 2678.1176758\n",
      "Epoch[39/50], Step [101/600], Total Loss: 13034.5605, Reconst Loss: 10444.1738, KL Div: 2590.3872070\n",
      "Epoch[39/50], Step [201/600], Total Loss: 12135.1309, Reconst Loss: 9653.8584, KL Div: 2481.2729492\n",
      "Epoch[39/50], Step [301/600], Total Loss: 12149.6729, Reconst Loss: 9594.9004, KL Div: 2554.7722168\n",
      "Epoch[39/50], Step [401/600], Total Loss: 12254.3232, Reconst Loss: 9782.4248, KL Div: 2471.8986816\n",
      "Epoch[39/50], Step [501/600], Total Loss: 12022.8457, Reconst Loss: 9400.6133, KL Div: 2622.2321777\n",
      "Epoch[40/50], Step [1/600], Total Loss: 11900.1895, Reconst Loss: 9400.1719, KL Div: 2500.0175781\n",
      "Epoch[40/50], Step [101/600], Total Loss: 12215.2246, Reconst Loss: 9693.7334, KL Div: 2521.4907227\n",
      "Epoch[40/50], Step [201/600], Total Loss: 12425.2002, Reconst Loss: 9707.6006, KL Div: 2717.5998535\n",
      "Epoch[40/50], Step [301/600], Total Loss: 12901.3584, Reconst Loss: 10196.1201, KL Div: 2705.2380371\n",
      "Epoch[40/50], Step [401/600], Total Loss: 12936.9072, Reconst Loss: 10343.0010, KL Div: 2593.9060059\n",
      "Epoch[40/50], Step [501/600], Total Loss: 12331.3652, Reconst Loss: 9735.7168, KL Div: 2595.6484375\n",
      "Epoch[41/50], Step [1/600], Total Loss: 12697.1201, Reconst Loss: 10120.3164, KL Div: 2576.8039551\n",
      "Epoch[41/50], Step [101/600], Total Loss: 12332.9258, Reconst Loss: 9716.1641, KL Div: 2616.7614746\n",
      "Epoch[41/50], Step [201/600], Total Loss: 11913.0801, Reconst Loss: 9383.7852, KL Div: 2529.2954102\n",
      "Epoch[41/50], Step [301/600], Total Loss: 12335.9219, Reconst Loss: 9704.2178, KL Div: 2631.7043457\n",
      "Epoch[41/50], Step [401/600], Total Loss: 12032.5684, Reconst Loss: 9420.0400, KL Div: 2612.5288086\n",
      "Epoch[41/50], Step [501/600], Total Loss: 12768.9844, Reconst Loss: 10132.2500, KL Div: 2636.7348633\n",
      "Epoch[42/50], Step [1/600], Total Loss: 12459.6934, Reconst Loss: 9775.4346, KL Div: 2684.2592773\n",
      "Epoch[42/50], Step [101/600], Total Loss: 12196.8145, Reconst Loss: 9707.5352, KL Div: 2489.2790527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[42/50], Step [201/600], Total Loss: 12549.2275, Reconst Loss: 9950.3555, KL Div: 2598.8720703\n",
      "Epoch[42/50], Step [301/600], Total Loss: 12140.4268, Reconst Loss: 9475.9053, KL Div: 2664.5212402\n",
      "Epoch[42/50], Step [401/600], Total Loss: 11913.6250, Reconst Loss: 9335.5068, KL Div: 2578.1186523\n",
      "Epoch[42/50], Step [501/600], Total Loss: 12333.9092, Reconst Loss: 9592.9170, KL Div: 2740.9921875\n",
      "Epoch[43/50], Step [1/600], Total Loss: 12447.9834, Reconst Loss: 9778.2012, KL Div: 2669.7824707\n",
      "Epoch[43/50], Step [101/600], Total Loss: 12699.3164, Reconst Loss: 10057.1016, KL Div: 2642.2150879\n",
      "Epoch[43/50], Step [201/600], Total Loss: 12614.4629, Reconst Loss: 10002.2061, KL Div: 2612.2570801\n",
      "Epoch[43/50], Step [301/600], Total Loss: 11777.5518, Reconst Loss: 9342.7617, KL Div: 2434.7902832\n",
      "Epoch[43/50], Step [401/600], Total Loss: 11665.8691, Reconst Loss: 9051.5928, KL Div: 2614.2761230\n",
      "Epoch[43/50], Step [501/600], Total Loss: 12098.1592, Reconst Loss: 9479.6318, KL Div: 2618.5270996\n",
      "Epoch[44/50], Step [1/600], Total Loss: 12402.3193, Reconst Loss: 9843.9160, KL Div: 2558.4035645\n",
      "Epoch[44/50], Step [101/600], Total Loss: 12382.2129, Reconst Loss: 9808.7275, KL Div: 2573.4858398\n",
      "Epoch[44/50], Step [201/600], Total Loss: 12358.0400, Reconst Loss: 9675.0449, KL Div: 2682.9951172\n",
      "Epoch[44/50], Step [301/600], Total Loss: 12554.0645, Reconst Loss: 9936.2646, KL Div: 2617.8000488\n",
      "Epoch[44/50], Step [401/600], Total Loss: 12947.4932, Reconst Loss: 10214.2822, KL Div: 2733.2106934\n",
      "Epoch[44/50], Step [501/600], Total Loss: 11972.5771, Reconst Loss: 9428.5430, KL Div: 2544.0341797\n",
      "Epoch[45/50], Step [1/600], Total Loss: 11976.2246, Reconst Loss: 9449.5557, KL Div: 2526.6684570\n",
      "Epoch[45/50], Step [101/600], Total Loss: 12414.0312, Reconst Loss: 9815.5078, KL Div: 2598.5231934\n",
      "Epoch[45/50], Step [201/600], Total Loss: 12202.0547, Reconst Loss: 9621.3945, KL Div: 2580.6601562\n",
      "Epoch[45/50], Step [301/600], Total Loss: 12392.6592, Reconst Loss: 9667.7617, KL Div: 2724.8977051\n",
      "Epoch[45/50], Step [401/600], Total Loss: 11680.1758, Reconst Loss: 9250.4336, KL Div: 2429.7424316\n",
      "Epoch[45/50], Step [501/600], Total Loss: 12007.0391, Reconst Loss: 9490.1318, KL Div: 2516.9067383\n",
      "Epoch[46/50], Step [1/600], Total Loss: 11736.3711, Reconst Loss: 9262.1250, KL Div: 2474.2463379\n",
      "Epoch[46/50], Step [101/600], Total Loss: 12071.6992, Reconst Loss: 9452.4893, KL Div: 2619.2104492\n",
      "Epoch[46/50], Step [201/600], Total Loss: 12094.9531, Reconst Loss: 9504.8652, KL Div: 2590.0874023\n",
      "Epoch[46/50], Step [301/600], Total Loss: 12147.6816, Reconst Loss: 9695.1621, KL Div: 2452.5190430\n",
      "Epoch[46/50], Step [401/600], Total Loss: 11693.1729, Reconst Loss: 9228.4971, KL Div: 2464.6755371\n",
      "Epoch[46/50], Step [501/600], Total Loss: 12271.2285, Reconst Loss: 9690.2920, KL Div: 2580.9365234\n",
      "Epoch[47/50], Step [1/600], Total Loss: 12592.6348, Reconst Loss: 9873.0947, KL Div: 2719.5395508\n",
      "Epoch[47/50], Step [101/600], Total Loss: 11853.1299, Reconst Loss: 9284.2725, KL Div: 2568.8574219\n",
      "Epoch[47/50], Step [201/600], Total Loss: 12518.0898, Reconst Loss: 9714.8213, KL Div: 2803.2683105\n",
      "Epoch[47/50], Step [301/600], Total Loss: 12048.1475, Reconst Loss: 9488.5068, KL Div: 2559.6406250\n",
      "Epoch[47/50], Step [401/600], Total Loss: 11801.3936, Reconst Loss: 9324.4678, KL Div: 2476.9260254\n",
      "Epoch[47/50], Step [501/600], Total Loss: 11910.4551, Reconst Loss: 9282.5127, KL Div: 2627.9426270\n",
      "Epoch[48/50], Step [1/600], Total Loss: 12105.4180, Reconst Loss: 9481.1182, KL Div: 2624.2998047\n",
      "Epoch[48/50], Step [101/600], Total Loss: 12507.6348, Reconst Loss: 9786.0654, KL Div: 2721.5695801\n",
      "Epoch[48/50], Step [201/600], Total Loss: 12441.5938, Reconst Loss: 9821.8652, KL Div: 2619.7287598\n",
      "Epoch[48/50], Step [301/600], Total Loss: 12257.7129, Reconst Loss: 9574.2686, KL Div: 2683.4448242\n",
      "Epoch[48/50], Step [401/600], Total Loss: 11515.2197, Reconst Loss: 9021.1172, KL Div: 2494.1022949\n",
      "Epoch[48/50], Step [501/600], Total Loss: 12587.3945, Reconst Loss: 9982.5469, KL Div: 2604.8476562\n",
      "Epoch[49/50], Step [1/600], Total Loss: 12203.3809, Reconst Loss: 9597.7646, KL Div: 2605.6159668\n",
      "Epoch[49/50], Step [101/600], Total Loss: 12706.0049, Reconst Loss: 9993.6387, KL Div: 2712.3664551\n",
      "Epoch[49/50], Step [201/600], Total Loss: 12747.5537, Reconst Loss: 10086.3428, KL Div: 2661.2111816\n",
      "Epoch[49/50], Step [301/600], Total Loss: 12501.8721, Reconst Loss: 9693.5176, KL Div: 2808.3547363\n",
      "Epoch[49/50], Step [401/600], Total Loss: 11955.0537, Reconst Loss: 9333.4316, KL Div: 2621.6218262\n",
      "Epoch[49/50], Step [501/600], Total Loss: 12258.7314, Reconst Loss: 9633.8721, KL Div: 2624.8596191\n",
      "Epoch[50/50], Step [1/600], Total Loss: 12289.9121, Reconst Loss: 9571.0068, KL Div: 2718.9052734\n",
      "Epoch[50/50], Step [101/600], Total Loss: 12152.3682, Reconst Loss: 9505.2891, KL Div: 2647.0793457\n",
      "Epoch[50/50], Step [201/600], Total Loss: 11816.5381, Reconst Loss: 9277.0625, KL Div: 2539.4758301\n",
      "Epoch[50/50], Step [301/600], Total Loss: 12196.2109, Reconst Loss: 9641.0693, KL Div: 2555.1411133\n",
      "Epoch[50/50], Step [401/600], Total Loss: 12473.7656, Reconst Loss: 9869.0869, KL Div: 2604.6791992\n",
      "Epoch[50/50], Step [501/600], Total Loss: 12053.6562, Reconst Loss: 9418.2773, KL Div: 2635.3784180\n"
     ]
    }
   ],
   "source": [
    "#Define model\n",
    "vae = ConvVAE()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    vae.cuda()\n",
    "    \n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
    "iter_per_epoch = len(data_loader)\n",
    "data_iter = iter(data_loader)\n",
    "\n",
    "# fixed inputs for debugging\n",
    "fixed_z = to_var(torch.randn(100, 20))\n",
    "fixed_x, _ = next(data_iter)\n",
    "torchvision.utils.save_image(fixed_x.cpu(), './data/real_images.png')\n",
    "fixed_x = to_var(fixed_x)\n",
    "\n",
    "for epoch in range(50):\n",
    "    for i, (images, _) in enumerate(data_loader):\n",
    "        \n",
    "        images = to_var(images)\n",
    "        out, mu, log_var = vae(images)\n",
    "        \n",
    "        # Compute reconstruction loss and kl divergence\n",
    "        reconst_loss = F.binary_cross_entropy(out, images, size_average=False)\n",
    "        kl_divergence = torch.sum(0.5 * (mu**2 + torch.exp(log_var) - log_var -1))\n",
    "        \n",
    "        # Backprop + Optimize\n",
    "        total_loss = reconst_loss + kl_divergence\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print (\"Epoch[%d/%d], Step [%d/%d], Total Loss: %.4f, \"\n",
    "                   \"Reconst Loss: %.4f, KL Div: %.7f\" \n",
    "                   %(epoch+1, 50, i+1, iter_per_epoch, total_loss.data[0], \n",
    "                     reconst_loss.data[0], kl_divergence.data[0]))\n",
    "    \n",
    "    # Save the reconstructed images\n",
    "    reconst_images, _, _ = vae(fixed_x)\n",
    "    reconst_images = reconst_images.resize(reconst_images.size(0), 1, 28, 28)\n",
    "    torchvision.utils.save_image(reconst_images.data.cpu(), \n",
    "        './data/Conv2D/reconst_images_%d.png' %(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
